{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 4 : Anticiper les besoins en consommation électrique de bâtiments\n",
    "*Pierre-Eloi Ragetly*\n",
    "\n",
    "Ce projet fait parti du parcours *DataScientist* d'OpenClassrooms.\n",
    "\n",
    "L'objectif principal est de trouver un modèle permettant de prédire **les émissions de CO2 et la consommation totale d’énergie de bâtiments non destinés à l'habitation.**\n",
    "\n",
    "Pour cela nous disposons des données de la ville de Seattle pour les années 2015 et 2016. Ces données sont à récupérer sur le site kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie III : Data modeling\n",
    "\n",
    "Ce notebook a pour but de présenter le travail effectué sur la modélisation.\n",
    "\n",
    "Nous commencerons par séparer notre jeu de données en deux parties distinctes:\n",
    "- Le **training set**, qui va permettre d'entrainer les différents modèles;\n",
    "- Le **testing set**, qui permettra de déterminer la performance du modèle finale.\n",
    "\n",
    "Pour ce faire, la méthode `train_test_split()` de la classe *sklearn.model_selection* sera utilisée en réservant 20% des données pour le jeu de test.\n",
    "\n",
    "Puis les modèles les plus courants seront entraînés et comparés afin de conserver les plus prometteurs. Au préalable, *une recherche par quadrillage* sera effectuée pour automatiser le choix des *hyperparamètres*, et les variables les plus pertinentes seront sélectionnées par **RFE** (Recursive Feature Elimination).\n",
    "\n",
    "Après sélection des modèles les plus performants, nous affinerons encore les hyperparamètres à l'aide d'une *recherche aléatoire* cette fois ci, et nous en profiterons pour tester la pertinence de la variable *EnergyStarScore*.\n",
    "\n",
    "Nous analyserons enfin les erreurs des modèles afin de déterminer s'il est pertinent d'utiliser une *méthode d'ensemble*, ie. combiner plusieurs modèles pour construire un modèle plus performant.\n",
    "\n",
    "Le modèle final obtenu, nous pourrons évaluer sa performance à l'aide du jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:24:48.923023Z",
     "start_time": "2020-07-08T06:24:48.353934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import des librairies usuelles\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:24:48.930016Z",
     "start_time": "2020-07-08T06:24:48.924660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change some default parameters of matplotlib using seaborn\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams.update({'axes.titleweight': 'bold'})\n",
    "sns.set(style='ticks')\n",
    "current_palette = sns.color_palette('RdBu')\n",
    "sns.set_palette(current_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:24:48.973424Z",
     "start_time": "2020-07-08T06:24:48.932543Z"
    }
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "data = (pd.read_csv('data/data_tr.csv')\n",
    "          .set_index('OSEBuildingID')\n",
    "          .drop(columns='ENERGYSTARScore'))\n",
    "data_star = (pd.read_csv('data/data_tr.csv')\n",
    "              .set_index('OSEBuildingID')\n",
    "              .dropna(subset=['ENERGYSTARScore']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Modéliser-la-consommation-totale-d’énergie\" data-toc-modified-id=\"Modéliser-la-consommation-totale-d’énergie-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Modéliser la consommation totale d’énergie</a></span><ul class=\"toc-item\"><li><span><a href=\"#Créer-un-jeu-de-test\" data-toc-modified-id=\"Créer-un-jeu-de-test-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Créer un jeu de test</a></span></li><li><span><a href=\"#Comparaison-des-modèles\" data-toc-modified-id=\"Comparaison-des-modèles-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Comparaison des modèles</a></span><ul class=\"toc-item\"><li><span><a href=\"#En-conservant-les-valeurs-par-défaut-des-hyperparamères\" data-toc-modified-id=\"En-conservant-les-valeurs-par-défaut-des-hyperparamères-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>En conservant les valeurs par défaut des hyperparamères</a></span></li><li><span><a href=\"#En-optimisant-les-hyperparamètres-via-des-recherches-aléatoires-et/ou-par-grille\" data-toc-modified-id=\"En-optimisant-les-hyperparamètres-via-des-recherches-aléatoires-et/ou-par-grille-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>En optimisant les hyperparamètres via des recherches aléatoires et/ou par grille</a></span></li></ul></li><li><span><a href=\"#Recherche-des-variables-les-plus-significatives\" data-toc-modified-id=\"Recherche-des-variables-les-plus-significatives-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Recherche des variables les plus significatives</a></span></li><li><span><a href=\"#Sélection-des-cinq-meilleurs-modèles\" data-toc-modified-id=\"Sélection-des-cinq-meilleurs-modèles-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Sélection des cinq meilleurs modèles</a></span></li><li><span><a href=\"#Modèle-final\" data-toc-modified-id=\"Modèle-final-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Modèle final</a></span></li><li><span><a href=\"#Vérifier-la-pertinence-de-Energy-Star-Score\" data-toc-modified-id=\"Vérifier-la-pertinence-de-Energy-Star-Score-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Vérifier la pertinence de Energy Star Score</a></span></li></ul></li><li><span><a href=\"#Modéliser-la-consommation-totale-d’énergie\" data-toc-modified-id=\"Modéliser-la-consommation-totale-d’énergie-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Modéliser la consommation totale d’énergie</a></span><ul class=\"toc-item\"><li><span><a href=\"#Créer-un-jeu-de-test\" data-toc-modified-id=\"Créer-un-jeu-de-test-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Créer un jeu de test</a></span></li><li><span><a href=\"#Comparaison-des-modèles\" data-toc-modified-id=\"Comparaison-des-modèles-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Comparaison des modèles</a></span></li><li><span><a href=\"#Recherche-des-variables-les-plus-significatives\" data-toc-modified-id=\"Recherche-des-variables-les-plus-significatives-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Recherche des variables les plus significatives</a></span></li><li><span><a href=\"#Sélection-des-cinq-meilleurs-modèles\" data-toc-modified-id=\"Sélection-des-cinq-meilleurs-modèles-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Sélection des cinq meilleurs modèles</a></span></li><li><span><a href=\"#Modèle-final\" data-toc-modified-id=\"Modèle-final-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Modèle final</a></span></li><li><span><a href=\"#Vérifier-la-pertinence-de-Energy-Star-Score\" data-toc-modified-id=\"Vérifier-la-pertinence-de-Energy-Star-Score-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Vérifier la pertinence de Energy Star Score</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modéliser la consommation totale d’énergie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir modéliser la consommation totale en énergie il faut au préalable déterminer quelle sera notra valeur cible. Sous nous avons quatre variables potentielles :\n",
    "- SiteEnergyUse(kBtu)\n",
    "- SiteEnergyUseWN(kBtu)\n",
    "- SiteEnergyUse(kBtu)_log\n",
    "- SiteEnergyUseWN(kBtu)_log\n",
    "\n",
    "Nous avons vu lors de l'ingénierie des variable qu'il était préférable de prendre la version log pour avoir une distribution se rapprochant d'une distribution normale. On peut donc déjà écarter les deux premières.\n",
    "\n",
    "Se pose ensuite la question de savoir s'il est préférable de garder la version normalisée ou non normalisée. Pour rappel, la version normalisée et la consommation corrigée en prenant comme référence la température des trentes dernières années. Alors que la version normalisée est la consommation moyenne sur les années 2015 et 2016. Dans le contexte de réchauffement climatique, il est fort à parier que la températures des prochaines années sera plus proche de celles de 2015 et 2016 que de la température des trentes dernières années.\n",
    "\n",
    "Nous prendrons donc la version non normalisée **SiteEnergyUse(kBtu)_log**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:24:49.012728Z",
     "start_time": "2020-07-08T06:24:48.975007Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.iloc[:, :-6].values\n",
    "y = data.loc[:, 'SiteEnergyUse(kBtu)_log'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre *random_state* permet de définir *le germe* (seed) du générateur de nombre aléatoires, afin qu'il génère toujours la même suite d'indices pseudo-aléatoires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour comparer les résultats obtenus avec les algorithmes de regression les plus courants (voir liste ci-dessous), nous utiliserons la librairie *Scikit-Learn* ainsi que la librairie *XGBoost*.\n",
    "- Régression Ridge\n",
    "- Régression Lasso\n",
    "- Elastic Net\n",
    "- Régression SVM linéaire\n",
    "- Régression SVM avec noyau\n",
    "- Régression kNN\n",
    "- Arbre de décision\n",
    "- Forêt aléatoire\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "- Perceptron multi-couches\n",
    "\n",
    "Nous utiliserons comme mesure de performance la RMSE par validation croisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En conservant les valeurs par défaut des hyperparamères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:24:52.105830Z",
     "start_time": "2020-07-08T06:24:49.013984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>1.271064</td>\n",
       "      <td>0.815615</td>\n",
       "      <td>-0.287782</td>\n",
       "      <td>1.675330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>1.301737</td>\n",
       "      <td>0.037429</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.000595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>1.281750</td>\n",
       "      <td>0.044597</td>\n",
       "      <td>0.030058</td>\n",
       "      <td>0.015660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVR</th>\n",
       "      <td>1.348145</td>\n",
       "      <td>0.788955</td>\n",
       "      <td>-0.385276</td>\n",
       "      <td>1.642602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.833117</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.588541</td>\n",
       "      <td>0.040997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.991973</td>\n",
       "      <td>0.044768</td>\n",
       "      <td>0.414600</td>\n",
       "      <td>0.077137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.871922</td>\n",
       "      <td>0.049904</td>\n",
       "      <td>0.549237</td>\n",
       "      <td>0.052139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.647185</td>\n",
       "      <td>0.011835</td>\n",
       "      <td>0.752250</td>\n",
       "      <td>0.012061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.626153</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.768118</td>\n",
       "      <td>0.013423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.651344</td>\n",
       "      <td>0.020638</td>\n",
       "      <td>0.748912</td>\n",
       "      <td>0.017389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>2.013997</td>\n",
       "      <td>1.079672</td>\n",
       "      <td>-2.005821</td>\n",
       "      <td>2.947653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               RMSE  RMSE_std        R2    R2_std\n",
       "Ridge                      1.271064  0.815615 -0.287782  1.675330\n",
       "Lasso                      1.301737  0.037429 -0.000678  0.000595\n",
       "ElasticNet                 1.281750  0.044597  0.030058  0.015660\n",
       "LinearSVR                  1.348145  0.788955 -0.385276  1.642602\n",
       "SVR                        0.833117  0.036242  0.588541  0.040997\n",
       "KNeighborsRegressor        0.991973  0.044768  0.414600  0.077137\n",
       "DecisionTreeRegressor      0.871922  0.049904  0.549237  0.052139\n",
       "RandomForestRegressor      0.647185  0.011835  0.752250  0.012061\n",
       "GradientBoostingRegressor  0.626153  0.019641  0.768118  0.013423\n",
       "XGBRegressor               0.651344  0.020638  0.748912  0.017389\n",
       "MLPRegressor               2.013997  1.079672 -2.005821  2.947653"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functions.ml_modeling import get_models\n",
    "from functions.ml_modeling import compare_models\n",
    "\n",
    "# Create models\n",
    "default_models = get_models(X_train, y_train)\n",
    "\n",
    "# Compare models\n",
    "compare_models(X_train, y_train, default_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En optimisant les hyperparamètres via des recherches aléatoires et/ou par grille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:27:51.919661Z",
     "start_time": "2020-07-08T06:24:52.107757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.954890</td>\n",
       "      <td>0.052254</td>\n",
       "      <td>0.460463</td>\n",
       "      <td>0.049179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>1.146331</td>\n",
       "      <td>0.135182</td>\n",
       "      <td>0.219630</td>\n",
       "      <td>0.158305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>0.959244</td>\n",
       "      <td>0.056865</td>\n",
       "      <td>0.455671</td>\n",
       "      <td>0.051165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVR</th>\n",
       "      <td>1.298493</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>-0.054854</td>\n",
       "      <td>0.571335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.780573</td>\n",
       "      <td>0.043703</td>\n",
       "      <td>0.637942</td>\n",
       "      <td>0.048712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.973904</td>\n",
       "      <td>0.040785</td>\n",
       "      <td>0.438066</td>\n",
       "      <td>0.051661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.734815</td>\n",
       "      <td>0.024689</td>\n",
       "      <td>0.680798</td>\n",
       "      <td>0.016623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.641131</td>\n",
       "      <td>0.013494</td>\n",
       "      <td>0.756885</td>\n",
       "      <td>0.011884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.616372</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.774813</td>\n",
       "      <td>0.019057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.615812</td>\n",
       "      <td>0.019145</td>\n",
       "      <td>0.775576</td>\n",
       "      <td>0.015342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.755391</td>\n",
       "      <td>0.046810</td>\n",
       "      <td>0.660108</td>\n",
       "      <td>0.052493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               RMSE  RMSE_std        R2    R2_std\n",
       "Ridge                      0.954890  0.052254  0.460463  0.049179\n",
       "Lasso                      1.146331  0.135182  0.219630  0.158305\n",
       "ElasticNet                 0.959244  0.056865  0.455671  0.051165\n",
       "LinearSVR                  1.298493  0.353942 -0.054854  0.571335\n",
       "SVR                        0.780573  0.043703  0.637942  0.048712\n",
       "KNeighborsRegressor        0.973904  0.040785  0.438066  0.051661\n",
       "DecisionTreeRegressor      0.734815  0.024689  0.680798  0.016623\n",
       "RandomForestRegressor      0.641131  0.013494  0.756885  0.011884\n",
       "GradientBoostingRegressor  0.616372  0.014684  0.774813  0.019057\n",
       "XGBRegressor               0.615812  0.019145  0.775576  0.015342\n",
       "MLPRegressor               0.755391  0.046810  0.660108  0.052493"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create models\n",
    "models = get_models(X_train, y_train, best_hparams=True)\n",
    "\n",
    "# Compare models\n",
    "df = compare_models(X_train, y_train, models)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des variables les plus significatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une grande qualité des forêts aléatoires est qu'elles permettent de mesurer facilement l'importance relative des variables. C'est donc tout naturellement que nous allons utiliser cet algorithme pour sélectionner les variables les plus pertinentes.\n",
    "\n",
    "Pour cela nous allons utiliser la classe **RFECV** de *sklearn.feature_selection*. La méthode RFE (Recursive Feature Elimination) permet de supprimer les variables de manière récursives. Le modèle est d'abord entrainé sur toutes les variables, puis l'algorithme écarte à chaque itération la ou les variables (depend du pas renseigné par l'utilisateur) les moins significatives jusqu'à obtenir le nombre de variable voulu. Pour déterminer le nombre de variables à conserver, on utilise une validation croisée.\n",
    "\n",
    "Ensuite nous ne conserverons que les variables qui sont significatives et réentrainerons les modèles avec ces variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que les meilleurs résultats sont obtenus lorsque nous conservons 29 variables sur les 54. Regardons ce qu'il se passe quand on entraine les modèles sur 29 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:30:51.561394Z",
     "start_time": "2020-07-08T06:27:51.921326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 features have been selected\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "min_features = int(X_train.shape[1]/5)\n",
    "forest_reg = [m for m in models if type(m).__name__=='RandomForestRegressor'][0]\n",
    "\n",
    "rfe = RFECV(forest_reg, min_features_to_select=min_features,\n",
    "            cv=cv)\n",
    "rfe.fit(X_train, y_train)\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "mask = rfe.support_\n",
    "\n",
    "print('{} features have been selected'.format(rfe.n_features_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que les meilleurs résultats sont obtenus lorsque nous conservons 48 variables sur les 54. Regardons ce qu'il se passe quand on entraine les modèles sur 48 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:33:51.639971Z",
     "start_time": "2020-07-08T06:30:51.564214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.954890</td>\n",
       "      <td>0.052254</td>\n",
       "      <td>0.460463</td>\n",
       "      <td>0.049179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>1.146331</td>\n",
       "      <td>0.135182</td>\n",
       "      <td>0.219630</td>\n",
       "      <td>0.158305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>0.959244</td>\n",
       "      <td>0.056865</td>\n",
       "      <td>0.455671</td>\n",
       "      <td>0.051165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVR</th>\n",
       "      <td>1.298493</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>-0.054854</td>\n",
       "      <td>0.571335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.780573</td>\n",
       "      <td>0.043703</td>\n",
       "      <td>0.637942</td>\n",
       "      <td>0.048712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.973904</td>\n",
       "      <td>0.040785</td>\n",
       "      <td>0.438066</td>\n",
       "      <td>0.051661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.734815</td>\n",
       "      <td>0.024689</td>\n",
       "      <td>0.680798</td>\n",
       "      <td>0.016623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.641131</td>\n",
       "      <td>0.013494</td>\n",
       "      <td>0.756885</td>\n",
       "      <td>0.011884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.616372</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.774813</td>\n",
       "      <td>0.019057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.615812</td>\n",
       "      <td>0.019145</td>\n",
       "      <td>0.775576</td>\n",
       "      <td>0.015342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.755391</td>\n",
       "      <td>0.046810</td>\n",
       "      <td>0.660108</td>\n",
       "      <td>0.052493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               RMSE  RMSE_std        R2    R2_std\n",
       "Ridge                      0.954890  0.052254  0.460463  0.049179\n",
       "Lasso                      1.146331  0.135182  0.219630  0.158305\n",
       "ElasticNet                 0.959244  0.056865  0.455671  0.051165\n",
       "LinearSVR                  1.298493  0.353942 -0.054854  0.571335\n",
       "SVR                        0.780573  0.043703  0.637942  0.048712\n",
       "KNeighborsRegressor        0.973904  0.040785  0.438066  0.051661\n",
       "DecisionTreeRegressor      0.734815  0.024689  0.680798  0.016623\n",
       "RandomForestRegressor      0.641131  0.013494  0.756885  0.011884\n",
       "GradientBoostingRegressor  0.616372  0.014684  0.774813  0.019057\n",
       "XGBRegressor               0.615812  0.019145  0.775576  0.015342\n",
       "MLPRegressor               0.755391  0.046810  0.660108  0.052493"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create models\n",
    "models_rfe = get_models(X_train_rfe, y_train, best_hparams=True)\n",
    "\n",
    "# Compare models\n",
    "df_rfe = compare_models(X_train_rfe, y_train, models_rfe)\n",
    "df_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont similaires, voir légèrement meilleurs pour certains modèles. Nous continuerons donc avec 29 variables, cela permettra de réduire les temps d'entrainement sans abaisser la performance pour autant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection des cinq meilleurs modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:33:51.662859Z",
     "start_time": "2020-07-08T06:33:51.642414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top five models are:\n",
      "XGBRegressor\n",
      "GradientBoostingRegressor\n",
      "RandomForestRegressor\n",
      "DecisionTreeRegressor\n",
      "MLPRegressor\n"
     ]
    }
   ],
   "source": [
    "# Get a score based on both RMSE and R2 scores\n",
    "df_rfe['RMSE_minmax'] = (df_rfe['RMSE']-df_rfe['RMSE'].min())/ \\\n",
    "                        (df_rfe['RMSE'].max()-df_rfe['RMSE'].min())\n",
    "df_rfe['R2_minmax'] = (df_rfe['R2']-df_rfe['R2'].min())/ \\\n",
    "                           (df_rfe['R2'].max()-df_rfe['R2'].min())\n",
    "df_rfe['Score'] = df_rfe['R2_minmax'] - df_rfe['RMSE_minmax']\n",
    "\n",
    "# Keep the 3 models with the largest score\n",
    "best_models_name = df_rfe['Score'].nlargest(5).index\n",
    "best_models = [m for m in models_rfe if type(m).__name__ in best_models_name]\n",
    "print(\"The top five models are:\")\n",
    "for m in best_models_name:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons réentrainé les modèles les plus prometteurs en ne conservant que les variables significatives, nous allons pouvoir obtenir le modèle finale.\n",
    "\n",
    "Pour ce modèle finale, nous allons reprendre l'astuce des modèles ensemblistes, nous allons chercher à obtenir le meilleur des modèles sélectionnés en les empilant de manière à former un nouveau modèle plus performant. Pour cela, nous utiliserons la classe **StackingRegressor** de *sklearn.ensemble*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:14.822499Z",
     "start_time": "2020-07-08T06:33:51.665041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.734815</td>\n",
       "      <td>0.024689</td>\n",
       "      <td>0.680798</td>\n",
       "      <td>0.016623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.641131</td>\n",
       "      <td>0.013494</td>\n",
       "      <td>0.756885</td>\n",
       "      <td>0.011884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.616372</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.774813</td>\n",
       "      <td>0.019057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.615812</td>\n",
       "      <td>0.019145</td>\n",
       "      <td>0.775576</td>\n",
       "      <td>0.015342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.755391</td>\n",
       "      <td>0.046810</td>\n",
       "      <td>0.660108</td>\n",
       "      <td>0.052493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StackingRegressor</th>\n",
       "      <td>0.605804</td>\n",
       "      <td>0.015737</td>\n",
       "      <td>0.782378</td>\n",
       "      <td>0.019733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               RMSE  RMSE_std        R2    R2_std\n",
       "DecisionTreeRegressor      0.734815  0.024689  0.680798  0.016623\n",
       "RandomForestRegressor      0.641131  0.013494  0.756885  0.011884\n",
       "GradientBoostingRegressor  0.616372  0.014684  0.774813  0.019057\n",
       "XGBRegressor               0.615812  0.019145  0.775576  0.015342\n",
       "MLPRegressor               0.755391  0.046810  0.660108  0.052493\n",
       "StackingRegressor          0.605804  0.015737  0.782378  0.019733"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Get a stacking ensemble of models\n",
    "estimators = []\n",
    "for m in best_models:\n",
    "    estimators.append((type(m).__name__, m))\n",
    "stack_reg = StackingRegressor(estimators=estimators,\n",
    "                              final_estimator=RidgeCV(alphas=np.logspace(-2, 2, 5)))\n",
    "\n",
    "# Get the results\n",
    "best_models.append(stack_reg)\n",
    "df = compare_models(X_train_rfe, y_train, best_models)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle ainsi créé est plus performant. Cependant, le gain de performance est bien faible comparé aux résultats obtenus avec le XGBoost, alors que le temps de calcul nécessaire est lui nettement supérieur (il faut entrainer six modèles au lieu d'un seul). C'est pour cette raison que le modèle XGBoost est préféré au modèle qui utilise la technique de l'empilement.\n",
    "\n",
    "Pour finir, regardons ce que donne le XGBoost sur les données de tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:51:10.555169Z",
     "start_time": "2020-07-08T06:51:10.550641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9112227338597874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "gboost = best_models[-4]\n",
    "y_pred = np.exp(gboost.predict(X_test[:, mask]))\n",
    "y_true = np.exp(y_test)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérifier la pertinence de Energy Star Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons pour clore cette partie consacrée à la modélisation des consommations énergétiques, vérifier la pertinence de la variable *Energy Star Score*. Pour cela nous allons entrainer le modèle XGBoost sur deux jeux données distincts uniquement par la présence (ou non) de cette variable. Puis nous comparerons les résultats obtenus avec chaque modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:15.041079Z",
     "start_time": "2020-07-08T06:24:48.370Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from functions.ml_modeling import gboost_reg_best_params\n",
    "\n",
    "\n",
    "# Split the dataset into random train and test subsets\n",
    "mask_star = (data_star.columns[:-6]!='ENERGYSTARScore')\n",
    "X_star = data_star.iloc[:, :-6].values\n",
    "y_star = data_star.loc[:, 'SiteEnergyUse(kBtu)_log'].values\n",
    "X_star_train, X_star_test, y_star_train, y_star_test = train_test_split(X_star,\n",
    "                                                                        y_star,\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        random_state=42)\n",
    "# Train the models\n",
    "gboost_star = gboost_reg_best_params(X_star_train,\n",
    "                                     y_star_train,\n",
    "                                     GradientBoostingRegressor())\n",
    "gboost_no_star = gboost_reg_best_params(X_star_train[:, mask_star],\n",
    "                                        y_star_train,\n",
    "                                        GradientBoostingRegressor())\n",
    "\n",
    "# Get results\n",
    "r2 = []\n",
    "y_pred = np.exp(gboost_star.predict(X_star_test))\n",
    "y_true = np.exp(y_star_test)\n",
    "r2.append(r2_score(y_true, y_pred))\n",
    "y_pred = np.exp(gboost_no_star.predict(X_star_test[:, mask_star]))\n",
    "r2.append(r2_score(y_true, y_pred))\n",
    "df_star = pd.DataFrame({'EnergyStarScore': [True, False], 'R2': r2})\n",
    "df_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que l'Energy Star Score permet bien d'améliorer le modèle. Cependant, bon nombre de bâtiments n'ont pas ce score. Or le résultat du 1.5 montre clairement qu'il est préférable de conserver un maximum de données, quitte à se passer de l'Energy Star Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modéliser la consommation totale d’énergie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là encore, nous utiliserons la version log pour l'entrainement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:15.042235Z",
     "start_time": "2020-07-08T06:24:48.372Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.iloc[:, :-6].values\n",
    "y = data.loc[:, 'TotalGHGEmissions_log'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:15.043282Z",
     "start_time": "2020-07-08T06:24:48.373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create models\n",
    "models = get_models(X_train, y_train, best_hparams=True)\n",
    "\n",
    "# Compare models\n",
    "df = compare_models(X_train, y_train, models)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des variables les plus significatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:15.044496Z",
     "start_time": "2020-07-08T06:24:48.374Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "min_features = int(X_train.shape[1]/5)\n",
    "forest_reg = [m for m in models if type(m).__name__=='RandomForestRegressor'][0]\n",
    "\n",
    "rfe = RFECV(forest_reg, min_features_to_select=min_features,\n",
    "            cv=cv)\n",
    "rfe.fit(X_train, y_train)\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "mask = rfe.support_\n",
    "\n",
    "print('{} features have been selected'.format(rfe.n_features_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:15.045730Z",
     "start_time": "2020-07-08T06:24:48.375Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create models\n",
    "models_rfe = get_models(X_train_rfe, y_train, best_hparams=True)\n",
    "\n",
    "# Compare models\n",
    "df_rfe = compare_models(X_train_rfe, y_train, models_rfe)\n",
    "df_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection des cinq meilleurs modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:15.046874Z",
     "start_time": "2020-07-08T06:24:48.377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a score based on both RMSE and R2 scores\n",
    "df_rfe['RMSE_minmax'] = (df_rfe['RMSE']-df_rfe['RMSE'].min())/ \\\n",
    "                        (df_rfe['RMSE'].max()-df_rfe['RMSE'].min())\n",
    "df_rfe['R2_minmax'] = (df_rfe['R2']-df_rfe['R2'].min())/ \\\n",
    "                           (df_rfe['R2'].max()-df_rfe['R2'].min())\n",
    "df_rfe['Score'] = df_rfe['R2_minmax'] - df_rfe['RMSE_minmax']\n",
    "\n",
    "# Keep the 3 models with the largest score\n",
    "best_models_name = df_rfe['Score'].nlargest(5).index\n",
    "best_models = [m for m in models_rfe if type(m).__name__ in best_models_name]\n",
    "print(\"The top five models are:\")\n",
    "for m in best_models_name:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:15.047765Z",
     "start_time": "2020-07-08T06:24:48.378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a stacking ensemble of models\n",
    "estimators = []\n",
    "for m in best_models:\n",
    "    estimators.append((type(m).__name__, m))\n",
    "stack_reg = StackingRegressor(estimators=estimators,\n",
    "                              final_estimator=RidgeCV(alphas=np.logspace(-2, 2, 5)))\n",
    "\n",
    "# Get the results\n",
    "best_models.append(stack_reg)\n",
    "df = compare_models(X_train_rfe, y_train, best_models)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:15.048689Z",
     "start_time": "2020-07-08T06:24:48.379Z"
    }
   },
   "outputs": [],
   "source": [
    "gboost = best_models[-4]\n",
    "y_pred = np.exp(gboost.predict(X_test[:, mask]))\n",
    "y_true = np.exp(y_test)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérifier la pertinence de Energy Star Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:34:15.049829Z",
     "start_time": "2020-07-08T06:24:48.381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset into random train and test subsets\n",
    "mask_star = (data_star.columns[:-6]!='ENERGYSTARScore')\n",
    "X_star = data_star.iloc[:, :-6].values\n",
    "y_star = data_star.loc[:, 'TotalGHGEmissions_log'].values\n",
    "X_star_train, X_star_test, y_star_train, y_star_test = train_test_split(X_star,\n",
    "                                                                        y_star,\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        random_state=42)\n",
    "# Train the models\n",
    "gboost_star = gboost_reg_best_params(X_star_train,\n",
    "                                     y_star_train,\n",
    "                                     GradientBoostingRegressor())\n",
    "gboost_no_star = gboost_reg_best_params(X_star_train[:, mask_star],\n",
    "                                        y_star_train,\n",
    "                                        GradientBoostingRegressor())\n",
    "\n",
    "# Get results\n",
    "r2 = []\n",
    "y_pred = np.exp(gboost_star.predict(X_star_test))\n",
    "y_true = np.exp(y_star_test)\n",
    "r2.append(r2_score(y_true, y_pred))\n",
    "y_pred = np.exp(xgboost_no_star.predict(X_star_test[:, mask_star]))\n",
    "r2.append(r2_score(y_true, y_pred))\n",
    "df_star = pd.DataFrame({'EnergyStarScore': [True, False], 'R2': r2})\n",
    "df_star"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "254px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
